{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_pipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOVZ4ePU1MT4m2Sswb652+5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d0pDRySinsay"},"outputs":[],"source":["!git lfs install\n","!git clone https://huggingface.co/oscarfossey/job_classification\n","!pip install pickle\n","!pip install spacy\n","!spacy download fr_core_news_sm\n","!pip install joblib"]},{"cell_type":"code","source":["global tokenizer_dl, tokenizer\n","tokenizer_dl = False\n","\n","def preprocessing_LSTM(texts_array):\n","    \"\"\"preprocessing the strings through the array to predict using the predict_tfidf function\n","    return an array of token inputs\"\"\"\n","\n","    import numpy as np\n","    import nltk\n","    import spacy\n","    import pickle\n","    nltk.download('stopwords')\n","\n","    stopwords = nltk.corpus.stopwords.words('french')\n","    nlp = spacy.load(\"fr_core_news_sm\")\n","    \n","    init_shape  = texts_array.shape\n","    \n","    def preprocess(text):\n","      text = text.lower()\n","      text = text.replace('(', ' ').replace(')', ' ').replace('.', ' ').replace('  ', ' ')  #drop '(', ')', '.'\n","      text = nlp(text)\n","      words = [token.lemma_ for sent in text.sents for token in sent if not token.text in set(stopwords)]\n","      string = ' '.join(words)\n","      return string\n","\n","    def tokenization_LSTM(new_offer):\n","      global tokenizer, tokenizer_dl\n","      \n","      from keras.preprocessing.text import Tokenizer\n","      from keras.preprocessing.sequence import pad_sequences\n","      MAX_SEQUENCE_LENGTH=250\n","      MAX_NB_WORDS = 50000\n","      if not(tokenizer_dl):\n","        # If to DL the tokenizer only one time\n","        tokenizer = pickle.load(open(\"/content/job_classification/LSTM_tokenizer\", 'rb'))\n","        tokenizer_dl = True\n","\n","      seq = tokenizer.texts_to_sequences([preprocess(new_offer)])\n","      padded = pad_sequences(seq, maxlen = MAX_SEQUENCE_LENGTH)\n","      return (padded)\n","\n","    token_inputs = np.array([tokenization_LSTM(preprocess(txt)) for txt in list(texts_array.flatten())])\n","\n","    return token_inputs"],"metadata":{"id":"evLS2NaunzfX","executionInfo":{"status":"ok","timestamp":1650760316483,"user_tz":-120,"elapsed":4,"user":{"displayName":"Oscar Fossey","userId":"10790742002274273934"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["global LSTM_model_dl, lstm_model\n","LSTM_model_dl = False\n","\n","def predict_LSTM(texts_array):\n","  \n","  from joblib import load\n","  import numpy as np\n","  global LSTM_model_dl, lstm_model\n","\n","  init_shape  = texts_array.shape\n","\n","  if not(LSTM_model_dl):\n","    lstm_model = load(open(\"/content/job_classification/model_LSTM.joblib\", 'rb')) \n","    LSTM_model_dl = True\n","\n","  predictions = np.array([lstm_model.predict(token_input)  for token_input in list(preprocessing_LSTM(texts_array.flatten()))])\n","  labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'N', 'M']\n","  predictions = np.array([labels[np.argmax(pred)] for pred in predictions])\n","  \n","  return predictions.reshape(init_shape)"],"metadata":{"id":"wcPtra6kn2xL","executionInfo":{"status":"ok","timestamp":1650760318373,"user_tz":-120,"elapsed":292,"user":{"displayName":"Oscar Fossey","userId":"10790742002274273934"}}},"execution_count":33,"outputs":[]}]}