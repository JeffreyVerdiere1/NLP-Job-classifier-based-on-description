{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"camembert_pipeline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vpaVissTAHeZ"},"outputs":[],"source":["!git lfs install\n","!git clone https://huggingface.co/oscarfossey/job_classification\n","!pip install pickle\n","!pip install spacy\n","!pip install keras\n","!spacy download fr_core_news_sm\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install datasets"]},{"cell_type":"code","source":["from transformers import CamembertTokenizer, CamembertForSequenceClassification\n","from keras.preprocessing.sequence import pad_sequences\n","global camembert_tokenizer, MAX_LEN\n","camembert_tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)\n","MAX_LEN = 200\n","\n","def preprocessing_camembert(texts_array):\n","  \"\"\"This functions takes an array of strings and return the inputs and the masks of the camemebert model\"\"\"\n","\n","  texts_list = list(texts_array.flatten())\n","  input_ids  = [camembert_tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN,truncation=True) for sent in texts_list]\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","  attention_masks = []\n","  for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]  \n","    attention_masks.append(seq_mask)\n","\n","  return (input_ids, attention_masks)"],"metadata":{"id":"Sf9oI0KH3i47","executionInfo":{"status":"ok","timestamp":1650794750696,"user_tz":-120,"elapsed":5272,"user":{"displayName":"Jeffrey Verdiere","userId":"15205464892636116192"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pickle\n","import numpy as np\n","from transformers import CamembertForSequenceClassification\n","from datasets import load_dataset\n","\n","labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'N', 'M']\n","\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if DEVICE.type != 'cpu':\n","  print(torch.cuda.get_device_name(DEVICE))\n","\n","camembert_model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=14) #for our 14 categories A to N\n","camembert_model.load_state_dict(torch.load(\"/content/job_classification/model_camembert_unbalancedv2.pth\"))\n","\n","if DEVICE.type != 'cpu':\n","  camembert_model.to(DEVICE)\n","\n","tags_name = load_dataset(\"oscarfossey/NLP_Pole_emploi\", data_files='tags_name.csv')\n","\n","def predict_camembert(texts_array, step = 100):\n","  \"Predicting the outputs step by step trough all the inputs\"\n","  \n","  inputs, masks = preprocessing_camembert(texts_array)\n","  inputs = torch.tensor(inputs)\n","  masks = torch.tensor(masks)\n","  camembert_model.eval()\n","  predictions = []\n","  i = 0\n","  while i < len(inputs) :\n","    pred = []      \n","    if DEVICE.type != 'cpu':\n","      local_inputs = inputs[i:min(i + step, len(inputs))].to(DEVICE)\n","      local_masks = masks[i:min(i + step, len(masks))].to(DEVICE)    \n","    else: \n","      local_inputs = inputs[i:min(i + step, len(inputs))]\n","      local_masks = masks[i:min(i + step, len(masks))]\n","    with torch.no_grad():\n","      outputs =  camembert_model(local_inputs, token_type_ids = None, attention_mask = local_masks)\n","      logits = outputs[0]\n","    if DEVICE.type != 'cpu':\n","      logits = logits.detach().cpu().numpy()\n","    pred.extend(np.argmax(logits, axis=1).flatten())\n","    predictions.extend(pred)\n","    i = min(i + step, len(inputs))\n","  \n","  predictions = [labels[i] for i in predictions]\n","      \n","  return predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3appL1C85svV","executionInfo":{"status":"ok","timestamp":1650794491680,"user_tz":-120,"elapsed":2065,"user":{"displayName":"Jeffrey Verdiere","userId":"15205464892636116192"}},"outputId":"b4d7a62c-9da9-4c1b-dd87-a1770e8f74fd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla P100-PCIE-16GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]}]}